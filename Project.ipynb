{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f4264-abf5-41e3-a9ec-19daf5c408d3",
   "metadata": {},
   "source": [
    "# Submitted by:\n",
    " \n",
    "* ID 1: 206664047\n",
    "* ID 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb4f12-7cc2-4689-8104-bb0b2113d2cc",
   "metadata": {},
   "source": [
    "# Warm up - Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c184a85-8705-400b-9a49-aa2921e60bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303da088-8b1e-427b-b347-e7f7baf9c26a",
   "metadata": {},
   "source": [
    "## Warmup - OOP in python\n",
    "\n",
    "Our desicion tree will be implemented using a dedicated python class. Python classes are very similar to classes in Java.\n",
    "\n",
    "You can use the following [site](https://jeffknupp.com/blog/2014/06/18/improve-your-python-python-classes-and-object-oriented-programming/) to learn about classes in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a60befc-f8c8-4c04-b59f-b319fcfbda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea6b867-3d69-4d37-8812-6ec90394520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = Node(5)\n",
    "p = Node(6)\n",
    "q = Node(7)\n",
    "n.add_child(p)\n",
    "n.add_child(q)\n",
    "n.children"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67acb976-20b3-40f4-8f81-fa48e2472bf4",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "For the following exercise, we will use a dataset containing mushroom data `agaricus-lepiota.csv`. \n",
    "\n",
    "This data set includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family. Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous\n",
    "one (=there are only two classes **edible** and **poisonous**). \n",
    "    \n",
    "The dataset contains 8124 observations with 22 features:\n",
    "1. cap-shape: bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s\n",
    "2. cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "3. cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "4. bruises: bruises=t,no=f\n",
    "5. odor: almond=a,anise=l,creosote=c,fishy=y,foul=f, musty=m,none=n,pungent=p,spicy=s\n",
    "6. gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "7. gill-spacing: close=c,crowded=w,distant=d\n",
    "8. gill-size: broad=b,narrow=n\n",
    "9. gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "10. stalk-shape: enlarging=e,tapering=t\n",
    "11. stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r\n",
    "12. stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "13. stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "14. stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "15. stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "16. veil-type: partial=p,universal=u\n",
    "17. veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "18. ring-number: none=n,one=o,two=t\n",
    "19. ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "20. spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "21. population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "22. habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dc7f5e-3eab-4374-89cb-e64272dfa994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('agaricus-lepiota.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e232a2-2f66-4ca1-b3d3-1402aa858691",
   "metadata": {},
   "source": [
    "One of the advantages of the Decision Tree algorithm is that almost no preprocessing is required. However, finding missing values is always required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9eb818-47db-434e-96ff-5f42986691df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# TODO: Find columns with missing values and remove them from the data.     #\n",
    "#############################################################################\n",
    "pass\n",
    "#############################################################################\n",
    "#                             END OF YOUR CODE                              #\n",
    "#############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc700567-3c21-46ec-8bbf-638005205941",
   "metadata": {},
   "source": [
    "We will split the dataset to `Training` and `Testing` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5504330-b81f-4551-90b4-9945b5fb881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Making sure the last column will hold the labels\n",
    "X, y = data.drop('class', axis=1), data['class']\n",
    "X = np.column_stack([X,y])\n",
    "# split dataset using random_state to get the same split each time\n",
    "X_train, X_test = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Testing dataset shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b42ab-33e2-43fe-b7b3-01cffaeaf250",
   "metadata": {},
   "source": [
    "## Impurity Measures\n",
    "\n",
    "Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Implement the functions `calc_gini` and `calc_entropy`. You are encouraged to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a11a0-b3b7-4f46-9a3b-a828dc14532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_gini(data):\n",
    "    \"\"\"\n",
    "    Calculate gini impurity measure of a dataset.\n",
    " \n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    " \n",
    "    Returns the gini impurity.    \n",
    "    \"\"\"\n",
    "    gini = 0.0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8425906-6166-401e-8b61-a63cf313b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    "\n",
    "    Returns the entropy of the dataset.    \n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ef5dd3-ab80-4fe3-970d-ff61d9df94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Your Tests Here #####\n",
    "calc_gini(X), calc_entropy(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb874b-20e9-4f9b-ad0d-4c9ee43c92ae",
   "metadata": {},
   "source": [
    "## Goodness of Split\n",
    "\n",
    "Given a feature the Goodnees of Split measures the reduction in the impurity if we split the data according to the feature.\n",
    "$$\n",
    "\\Delta\\varphi(S, A) = \\varphi(S) - \\sum_{v\\in Values(A)} \\frac{|S_v|}{|S|}\\varphi(S_v)\n",
    "$$\n",
    "\n",
    "NOTE: you can add more parameters to the function and you can also add more returning variables (The given parameters and the given returning variable should not be touch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c367f40-3f79-4741-94bf-a44c0bcfa6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodness_of_split(data, feature, impurity_func):\n",
    "    \"\"\"\n",
    "    Calculate the goodness of split of a dataset given a feature and impurity function.\n",
    "\n",
    "    Input:\n",
    "    - data: any dataset where the last column holds the labels.\n",
    "    - feature: the feature index.\n",
    "    - impurity func: a function that calculates the impurity.\n",
    "\n",
    "    Returns the goodness of split.  \n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return goodness    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135a4ee-399a-4aa1-829b-23446d8c5fa5",
   "metadata": {},
   "source": [
    "## Building a Decision Tree\n",
    "\n",
    "Use a Python class to construct the decision tree. Your class should support the following functionality:\n",
    "\n",
    "1. Initiating a node for a decision tree. You will need to use several class methods and class attributes and you are free to use them as you see fit. We recommend that every node will hold the feature and value used for the split and its children.\n",
    "2. Your code should support both Gini and Entropy as impurity measures. \n",
    "3. The provided data includes categorical data. In this exercise, when splitting a node create the number of children needed according to the attribute unique values.\n",
    "\n",
    "Complete the class `DecisionNode`. The structure of this class is entirely up to you. \n",
    "\n",
    "Complete the function `build_tree`. This function should get the training dataset and the impurity as inputs, initiate a root for the decision tree and construct the tree according to the procedure you learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860cf36-d241-49fb-92c7-c1c91e00a540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode:\n",
    "    \"\"\"\n",
    "    This class will hold everything you require to construct a decision tree.\n",
    "    The structure of this class is up to you. However, you need to support basic \n",
    "    functionality as described above. It is highly recommended that you \n",
    "    first read and understand the entire exercise before diving into this class.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature):\n",
    "        self.feature = feature # column index of criteria being tested\n",
    "        \n",
    "    def add_child(self, node):\n",
    "        self.children.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05e4f9b-04e8-4113-a438-0d9e11a8f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(data, impurity, min_samples_split=1, max_depth=1000):\n",
    "    \"\"\"\n",
    "    Build a tree using the given impurity measure and training dataset. \n",
    "    You are required to fully grow the tree until all leaves are pure. \n",
    "\n",
    "    Input:\n",
    "    - data: the training dataset.\n",
    "    - impurity: the chosen impurity measure. Notice that you can send a function\n",
    "                as an argument in python.\n",
    "    - min_samples_split: the minimum number of samples required to split an internal node (implementation of this feature is optional)\n",
    "    - max_depth: the allowable depth of the tree\n",
    "\n",
    "    Output: the root node of the tree.\n",
    "    \"\"\"\n",
    "    root = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f33ad4-3185-4c8a-94dc-9e771ab27498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python supports passing a function as an argument to another function.\n",
    "tree_gini = build_tree(data=X_train, impurity=calc_gini) # gini and goodness of split\n",
    "tree_entropy = build_tree(data=X_train, impurity=calc_entropy) # entropy and goodness of split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d89b1-e416-4029-b88f-8b6b37b9614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, instance):\n",
    "    \"\"\"\n",
    "    Predict a given instance using the decision tree\n",
    " \n",
    "    Input:\n",
    "    - root: the root of the decision tree.\n",
    "    - instance: an row vector from the dataset. Note that the last element \n",
    "                of this vector is the label of the instance.\n",
    " \n",
    "    Output: the prediction of the instance.\n",
    "    \"\"\"\n",
    "    pred = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return node.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce7c50-c0f7-41c5-a31b-2fd0c4314e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(node, dataset):\n",
    "    \"\"\"\n",
    "    Predict a given dataset using the decision tree\n",
    " \n",
    "    Input:\n",
    "    - node: a node in the decision tree.\n",
    "    - dataset: the dataset on which the accuracy is evaluated\n",
    " \n",
    "    Output: the accuracy of the decision tree on the given dataset (%).\n",
    "    \"\"\"\n",
    "    accuracy = 0\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af65dc-fb85-4c39-8a09-7510f0f26156",
   "metadata": {},
   "source": [
    "After building the three trees using the training set, you should calculate the accuracy on the test set. For each tree print the training and test accuracy. Select the tree that gave you the best test accuracy. For the rest of the exercise, use that tree (when you asked to build another tree use the same impurity function). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacec545-fff7-4e88-87df-2c196fd2c2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19516cd9-b85b-47d0-89d0-a2a37bd5f335",
   "metadata": {},
   "source": [
    "## Depth pruning\n",
    "\n",
    "Consider the following max_depth values: [1, 2, 3, 4, 5, 6, 7, 8]. For each value, construct a tree and prune it according to the max_depth value = don't let the tree to grow beyond this depth. Next, calculate the training and testing accuracy.<br>\n",
    "On a single plot, draw the training and testing accuracy as a function of the max_depth. Mark the best result on the graph with red circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce34f70-2738-4ba0-8e4a-8d606c24d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fa90f0-46f5-4ec2-96dd-a4736724aae6",
   "metadata": {},
   "source": [
    "# Part 2 - Titanic Survival Prediction\n",
    "\n",
    "In this part we will attempt to predict who survived on the Titanic. The data set we use has the following vairbales (features/attributes):\n",
    "\n",
    "\n",
    "|  Variable   |          Definition          |              Key/Values                 |\n",
    "|:-----------:|:----------------------------:|:---------------------------------------:|\n",
    "| PassengerId | Index                        |integer                                  |\n",
    "| Pclass      | Ticket class                 |1=1st, 2=2nd, 3=3rd                      |\n",
    "| Name        | Name of passenger            |string                                   |\n",
    "| Sex         | Sex                          |male, female                             |\n",
    "| Age         | Age in years                 |integer                                  |\n",
    "| SibSp       | # of siblings/spouses aboard |integer                                  |\n",
    "| Parch       | # of parents/children aboard |integer                                  |\n",
    "| Ticket      | Ticket number                |string                                   |\n",
    "| Fare        | Ticket fare                  |float                                    |\n",
    "| Cabin       | Cabin number                 |a code                                   |\n",
    "| Embarked    | Port of Embarkation          |C=Cherbourg, Q=Queenstown, S=Southampton |\n",
    "| **Survived**| Predicted varibale           |0=No, 1=Yes                              |\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "The goal is to produce the best score possible.\n",
    "\n",
    "**Methodogology**\n",
    "\n",
    "You can use every classification algorithm you saw in class. You may want to choose which features to use (it could be that some features are not useful). Further, some features have missing values. You will need to decide how to handle this (for instance: drop rows with missing values, place an avergae value in those rows, or some other method of your choosing). Another matter to consider is handling non-numeric values. For example, sex is non-numeric. You may choose to drop non-numeric features, or you could convert them to numeric values (if such conversion makes sense).\n",
    "Also you will need to consider splitting the data into a training set and a test set so as to avoid tailoring the solution (overfitting) to the data you have.\n",
    "\n",
    "Bottom line: use everything we talked about in class in order to learn the best model.\n",
    "\n",
    "**Model Output**\n",
    "\n",
    "Your model needs to produce a prediciton for each data sample (row), which is 0 (did not survive) or 1 (survived). \n",
    "\n",
    "**Scoring your model**\n",
    "\n",
    "Your model performance will be assessed on test data that is not available to you (you will see the score next semester). Try to avoid overfitting.\n",
    "\n",
    "**Final Note**\n",
    "\n",
    "This part of the project is open ended, in that you are not given small and specific tasks. However, you are already familiar with all the components needed to succeed. Specifically, reading data into pandas dataframe, dropping columns, dropping rows, changing value of features, splitting data into train and test subsets and performing model training and hyper parameter tuning via cross validation using sklearn library.\n",
    "\n",
    "GOOD LUCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e74df5-a029-4647-bd92-b881335ab408",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
